{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YXfJ10EsVseC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import glob\n",
        "import torch, os\n",
        "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast, TrainingArguments, Trainer, DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMPsihmGt5vi",
        "outputId": "508f2fcb-819b-496e-f15f-0eac55cd565a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('CPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1Fld25-kpot"
      },
      "outputs": [],
      "source": [
        "train_paths = ['/content/train/california_wildfires_2018_train.tsv',\n",
        "               '/content/train/canada_wildfires_2016_train.tsv',\n",
        "               '/content/train/cyclone_idai_2019_train.tsv',\n",
        "               '/content/train/ecuador_earthquake_2016_train.tsv',\n",
        "               '/content/train/greece_wildfires_2018_train.tsv',\n",
        "               '/content/train/hurricane_dorian_2019_train.tsv',\n",
        "               '/content/train/hurricane_florence_2018_train.tsv',\n",
        "               '/content/train/hurricane_harvey_2017_train.tsv',\n",
        "               '/content/train/hurricane_irma_2017_train.tsv',\n",
        "               '/content/train/hurricane_maria_2017_train.tsv',\n",
        "               '/content/train/hurricane_matthew_2016_train.tsv',\n",
        "               '/content/train/italy_earthquake_aug_2016_train.tsv',\n",
        "               '/content/train/kaikoura_earthquake_2016_train.tsv',\n",
        "               '/content/train/kerala_floods_2018_train.tsv',\n",
        "               '/content/train/maryland_floods_2018_train.tsv',\n",
        "               '/content/train/midwestern_us_floods_2019_train.tsv',\n",
        "               '/content/train/pakistan_earthquake_2019_train.tsv',\n",
        "               '/content/train/puebla_mexico_earthquake_2017_train.tsv',\n",
        "               '/content/train/srilanka_floods_2017_train.tsv']\n",
        "\n",
        "val_paths = ['/content/dev/california_wildfires_2018_dev.tsv',\n",
        "             '/content/dev/canada_wildfires_2016_dev.tsv',\n",
        "             '/content/dev/cyclone_idai_2019_dev.tsv',\n",
        "             '/content/dev/ecuador_earthquake_2016_dev.tsv',\n",
        "             '/content/dev/greece_wildfires_2018_dev.tsv',\n",
        "             '/content/dev/hurricane_dorian_2019_dev.tsv',\n",
        "             '/content/dev/hurricane_florence_2018_dev.tsv',\n",
        "             '/content/dev/hurricane_harvey_2017_dev.tsv',\n",
        "             '/content/dev/hurricane_irma_2017_dev.tsv',\n",
        "             '/content/dev/hurricane_maria_2017_dev.tsv',\n",
        "             '/content/dev/hurricane_matthew_2016_dev.tsv',\n",
        "             '/content/dev/italy_earthquake_aug_2016_dev.tsv',\n",
        "             '/content/dev/kaikoura_earthquake_2016_dev.tsv',\n",
        "             '/content/dev/kerala_floods_2018_dev.tsv',\n",
        "             '/content/dev/maryland_floods_2018_dev.tsv',\n",
        "             '/content/dev/midwestern_us_floods_2019_dev.tsv',\n",
        "             '/content/dev/pakistan_earthquake_2019_dev.tsv',\n",
        "             '/content/dev/puebla_mexico_earthquake_2017_dev.tsv',\n",
        "             '/content/dev/srilanka_floods_2017_dev.tsv']\n",
        "\n",
        "test_paths = ['/content/test/california_wildfires_2018_test.tsv',\n",
        "              '/content/test/canada_wildfires_2016_test.tsv',\n",
        "              '/content/test/cyclone_idai_2019_test.tsv',\n",
        "              '/content/test/ecuador_earthquake_2016_test.tsv',\n",
        "              '/content/test/greece_wildfires_2018_test.tsv',\n",
        "              '/content/test/hurricane_dorian_2019_test.tsv',\n",
        "              '/content/test/hurricane_florence_2018_test.tsv',\n",
        "              '/content/test/hurricane_harvey_2017_test.tsv',\n",
        "              '/content/test/hurricane_irma_2017_test.tsv',\n",
        "              '/content/test/hurricane_maria_2017_test.tsv',\n",
        "              '/content/test/hurricane_matthew_2016_test.tsv',\n",
        "              '/content/test/italy_earthquake_aug_2016_test.tsv',\n",
        "              '/content/test/kaikoura_earthquake_2016_test.tsv',\n",
        "              '/content/test/kerala_floods_2018_test.tsv',\n",
        "              '/content/test/maryland_floods_2018_test.tsv',\n",
        "              '/content/test/midwestern_us_floods_2019_test.tsv',\n",
        "              '/content/test/pakistan_earthquake_2019_test.tsv',\n",
        "              '/content/test/puebla_mexico_earthquake_2017_test.tsv',\n",
        "              '/content/test/srilanka_floods_2017_test.tsv']\n",
        "\n",
        "train_file_paths = []\n",
        "\n",
        "for path in train_paths:\n",
        "    files = glob.glob(path)\n",
        "    train_file_paths.extend(files)\n",
        "\n",
        "val_file_paths = []\n",
        "\n",
        "for path in val_paths:\n",
        "    files = glob.glob(path)\n",
        "    val_file_paths.extend(files)\n",
        "\n",
        "test_file_paths = []\n",
        "\n",
        "for path in test_paths:\n",
        "    files = glob.glob(path)\n",
        "    test_file_paths.extend(files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "fH8nhW9qkrhF",
        "outputId": "0ad78e95-080a-4a13-efca-8ceaef1d3368"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-16ebec9864e3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mval_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# dataframes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    373\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "train_dfs = []\n",
        "\n",
        "for file in train_file_paths:\n",
        "    df = pd.read_csv(file, sep='\\t')\n",
        "    df = df.iloc[:, 1:] # removing tweet ids\n",
        "    train_dfs.append(df)\n",
        "\n",
        "train_df = pd.concat(train_dfs, ignore_index=True)\n",
        "\n",
        "val_dfs = [] # dataframes\n",
        "\n",
        "for file in val_file_paths:\n",
        "    df = pd.read_csv(file, sep='\\t')\n",
        "    df = df.iloc[:, 1:] # removing tweet ids\n",
        "    val_dfs.append(df)\n",
        "\n",
        "val_df = pd.concat(val_dfs, ignore_index=True)\n",
        "\n",
        "test_dfs = [] # dataframes\n",
        "\n",
        "for file in test_file_paths:\n",
        "    df = pd.read_csv(file, sep='\\t')\n",
        "    df = df.iloc[:, 1:] # removing tweet ids\n",
        "    test_dfs.append(df)\n",
        "\n",
        "test_df = pd.concat(test_dfs, ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSWapyUNmWag"
      },
      "outputs": [],
      "source": [
        "class_labels = train_df['class_label'].unique().tolist()\n",
        "class_labels = [s.strip() for s in class_labels]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_UF9J1Bnqn1"
      },
      "outputs": [],
      "source": [
        "id2label = {id:label for id, label in enumerate(class_labels)}\n",
        "label2id = {label:id for id, label in enumerate(class_labels)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "91GZgvsHofrU"
      },
      "outputs": [],
      "source": [
        "train_df['class_label_num'] = pd.factorize(train_df.class_label)[0] # 53531\n",
        "val_df['class_label_num'] = pd.factorize(val_df.class_label)[0] # 7793\n",
        "test_df['class_label_num'] = pd.factorize(test_df.class_label)[0] # 15160"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a7bujVwOzZh_"
      },
      "outputs": [],
      "source": [
        "train_df.class_label.value_counts().plot(kind='pie', figsize=(10,10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQbGo4mq0qPH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARH8rD7KrtqH"
      },
      "outputs": [],
      "source": [
        "tokenizer = BertTokenizerFast.from_pretrained('google-bert/bert-base-uncased', max_length=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-V_J_C-VtT1m"
      },
      "outputs": [],
      "source": [
        "model = BertForSequenceClassification.from_pretrained('google-bert/bert-base-uncased', num_labels=len(class_labels), id2label=id2label, label2id=label2id)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn6anaaJ0hbW"
      },
      "outputs": [],
      "source": [
        "train_tweets = list(train_df.tweet_text)\n",
        "train_labels = list(train_df.class_label_num)\n",
        "\n",
        "val_tweets = list(val_df.tweet_text)\n",
        "val_labels = list(val_df.class_label_num)\n",
        "\n",
        "test_tweets = list(test_df.tweet_text)\n",
        "test_labels = list(test_df.class_label_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScSbBgGP8JHK"
      },
      "outputs": [],
      "source": [
        "print(len(train_tweets), len(val_tweets), len(test_tweets))\n",
        "print(len(train_labels), len(val_labels), len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8oiOSinuBiP"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_tweets, truncation=True, padding=True)\n",
        "val_encodings  = tokenizer(val_tweets, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(test_tweets, truncation=True, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKa4NIje1KQN"
      },
      "outputs": [],
      "source": [
        "class DataLoader(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgP3r5JY13ao"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(train_encodings, train_labels)\n",
        "\n",
        "val_dataloader = DataLoader(val_encodings, val_labels)\n",
        "\n",
        "test_dataloader = DataLoader(test_encodings, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqAfd6E82N-c"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'Accuracy': acc,\n",
        "        'F1': f1,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "RQqdL6ID2tBv",
        "outputId": "2fabb821-a077-4a33-e7a9-13243370a068"
      },
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-0bf44616ae21>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./preds_checkpoints'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdo_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdo_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnum_train_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp...\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1527\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"npu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"xpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1993\u001b[0m         \"\"\"\n\u001b[1;32m   1994\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1905\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   1906\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1907\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='./preds_checkpoints',\n",
        "    do_train=True,\n",
        "    do_eval=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy='steps',\n",
        "    logging_dir='./multi-class-logs',\n",
        "    logging_steps=50,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qirQAVws3e44",
        "outputId": "f42adcd4-2b5b-440c-cbee-71c595231762"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-76b951152ba0>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer = Trainer(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0meval_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataloader,\n",
        "    eval_dataset=val_dataloader,\n",
        "    compute_metrics= compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fIl7w3-23i2f",
        "outputId": "dbfa5c26-d9ba-4bc2-9859-55317205936e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2301' max='10038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2301/10038 09:32 < 32:07, 4.01 it/s, Epoch 0.69/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.656000</td>\n",
              "      <td>3.500642</td>\n",
              "      <td>0.303734</td>\n",
              "      <td>0.127655</td>\n",
              "      <td>0.133671</td>\n",
              "      <td>0.142059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.780900</td>\n",
              "      <td>3.496252</td>\n",
              "      <td>0.283973</td>\n",
              "      <td>0.128478</td>\n",
              "      <td>0.138155</td>\n",
              "      <td>0.136802</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.661300</td>\n",
              "      <td>3.563360</td>\n",
              "      <td>0.281535</td>\n",
              "      <td>0.127709</td>\n",
              "      <td>0.133143</td>\n",
              "      <td>0.128289</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.500900</td>\n",
              "      <td>3.724212</td>\n",
              "      <td>0.289747</td>\n",
              "      <td>0.124982</td>\n",
              "      <td>0.128620</td>\n",
              "      <td>0.137257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.609900</td>\n",
              "      <td>3.593887</td>\n",
              "      <td>0.293340</td>\n",
              "      <td>0.126550</td>\n",
              "      <td>0.129790</td>\n",
              "      <td>0.128931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.576700</td>\n",
              "      <td>3.688752</td>\n",
              "      <td>0.285898</td>\n",
              "      <td>0.129870</td>\n",
              "      <td>0.135478</td>\n",
              "      <td>0.131966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.574300</td>\n",
              "      <td>3.837301</td>\n",
              "      <td>0.280380</td>\n",
              "      <td>0.126393</td>\n",
              "      <td>0.136740</td>\n",
              "      <td>0.133150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.559300</td>\n",
              "      <td>3.798586</td>\n",
              "      <td>0.304761</td>\n",
              "      <td>0.130917</td>\n",
              "      <td>0.131425</td>\n",
              "      <td>0.151993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.515200</td>\n",
              "      <td>3.926718</td>\n",
              "      <td>0.269216</td>\n",
              "      <td>0.123712</td>\n",
              "      <td>0.132138</td>\n",
              "      <td>0.130752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.514000</td>\n",
              "      <td>3.866246</td>\n",
              "      <td>0.294110</td>\n",
              "      <td>0.124984</td>\n",
              "      <td>0.125650</td>\n",
              "      <td>0.127755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.454400</td>\n",
              "      <td>4.009865</td>\n",
              "      <td>0.288207</td>\n",
              "      <td>0.129245</td>\n",
              "      <td>0.137980</td>\n",
              "      <td>0.132929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.435700</td>\n",
              "      <td>4.003787</td>\n",
              "      <td>0.292699</td>\n",
              "      <td>0.128040</td>\n",
              "      <td>0.135996</td>\n",
              "      <td>0.128931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.462600</td>\n",
              "      <td>3.965183</td>\n",
              "      <td>0.311048</td>\n",
              "      <td>0.131484</td>\n",
              "      <td>0.135176</td>\n",
              "      <td>0.154046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.560100</td>\n",
              "      <td>3.856640</td>\n",
              "      <td>0.288977</td>\n",
              "      <td>0.126587</td>\n",
              "      <td>0.136022</td>\n",
              "      <td>0.132446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>3.815535</td>\n",
              "      <td>0.298986</td>\n",
              "      <td>0.128858</td>\n",
              "      <td>0.137345</td>\n",
              "      <td>0.138233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.480900</td>\n",
              "      <td>3.831487</td>\n",
              "      <td>0.292955</td>\n",
              "      <td>0.132173</td>\n",
              "      <td>0.140321</td>\n",
              "      <td>0.143464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.446600</td>\n",
              "      <td>4.019991</td>\n",
              "      <td>0.275760</td>\n",
              "      <td>0.123397</td>\n",
              "      <td>0.130985</td>\n",
              "      <td>0.125684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.603900</td>\n",
              "      <td>3.939375</td>\n",
              "      <td>0.296933</td>\n",
              "      <td>0.130253</td>\n",
              "      <td>0.135962</td>\n",
              "      <td>0.141919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>3.648540</td>\n",
              "      <td>0.296677</td>\n",
              "      <td>0.127903</td>\n",
              "      <td>0.133622</td>\n",
              "      <td>0.142939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.752600</td>\n",
              "      <td>3.518366</td>\n",
              "      <td>0.293468</td>\n",
              "      <td>0.127474</td>\n",
              "      <td>0.133540</td>\n",
              "      <td>0.140300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.727800</td>\n",
              "      <td>3.569819</td>\n",
              "      <td>0.301168</td>\n",
              "      <td>0.125330</td>\n",
              "      <td>0.130321</td>\n",
              "      <td>0.135220</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.694200</td>\n",
              "      <td>3.740076</td>\n",
              "      <td>0.298088</td>\n",
              "      <td>0.124098</td>\n",
              "      <td>0.129537</td>\n",
              "      <td>0.129622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.709600</td>\n",
              "      <td>3.711720</td>\n",
              "      <td>0.299628</td>\n",
              "      <td>0.125868</td>\n",
              "      <td>0.131818</td>\n",
              "      <td>0.135451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.718000</td>\n",
              "      <td>3.564658</td>\n",
              "      <td>0.301168</td>\n",
              "      <td>0.130132</td>\n",
              "      <td>0.135535</td>\n",
              "      <td>0.138412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.703500</td>\n",
              "      <td>3.728903</td>\n",
              "      <td>0.297831</td>\n",
              "      <td>0.125683</td>\n",
              "      <td>0.132630</td>\n",
              "      <td>0.135618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.656800</td>\n",
              "      <td>3.779703</td>\n",
              "      <td>0.281663</td>\n",
              "      <td>0.124636</td>\n",
              "      <td>0.135345</td>\n",
              "      <td>0.121288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.749500</td>\n",
              "      <td>3.651145</td>\n",
              "      <td>0.296420</td>\n",
              "      <td>0.126331</td>\n",
              "      <td>0.132072</td>\n",
              "      <td>0.134282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.659000</td>\n",
              "      <td>3.735197</td>\n",
              "      <td>0.289362</td>\n",
              "      <td>0.124841</td>\n",
              "      <td>0.133574</td>\n",
              "      <td>0.132858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.708000</td>\n",
              "      <td>3.605745</td>\n",
              "      <td>0.295650</td>\n",
              "      <td>0.122786</td>\n",
              "      <td>0.126281</td>\n",
              "      <td>0.130203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.640300</td>\n",
              "      <td>3.751348</td>\n",
              "      <td>0.294623</td>\n",
              "      <td>0.125787</td>\n",
              "      <td>0.128331</td>\n",
              "      <td>0.133250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.702900</td>\n",
              "      <td>3.673528</td>\n",
              "      <td>0.280765</td>\n",
              "      <td>0.124117</td>\n",
              "      <td>0.129799</td>\n",
              "      <td>0.124995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.728500</td>\n",
              "      <td>3.654963</td>\n",
              "      <td>0.282433</td>\n",
              "      <td>0.124905</td>\n",
              "      <td>0.129411</td>\n",
              "      <td>0.124716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.681000</td>\n",
              "      <td>3.817043</td>\n",
              "      <td>0.283460</td>\n",
              "      <td>0.124707</td>\n",
              "      <td>0.130474</td>\n",
              "      <td>0.126598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.651800</td>\n",
              "      <td>3.640150</td>\n",
              "      <td>0.295393</td>\n",
              "      <td>0.125760</td>\n",
              "      <td>0.131621</td>\n",
              "      <td>0.132421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.700700</td>\n",
              "      <td>3.674015</td>\n",
              "      <td>0.299500</td>\n",
              "      <td>0.120751</td>\n",
              "      <td>0.120134</td>\n",
              "      <td>0.130105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.672100</td>\n",
              "      <td>3.846841</td>\n",
              "      <td>0.286539</td>\n",
              "      <td>0.126048</td>\n",
              "      <td>0.131843</td>\n",
              "      <td>0.130892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.695400</td>\n",
              "      <td>3.638715</td>\n",
              "      <td>0.298601</td>\n",
              "      <td>0.122369</td>\n",
              "      <td>0.126723</td>\n",
              "      <td>0.129917</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.664100</td>\n",
              "      <td>3.801407</td>\n",
              "      <td>0.292057</td>\n",
              "      <td>0.125402</td>\n",
              "      <td>0.130531</td>\n",
              "      <td>0.133955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.633400</td>\n",
              "      <td>3.735868</td>\n",
              "      <td>0.286539</td>\n",
              "      <td>0.129171</td>\n",
              "      <td>0.139975</td>\n",
              "      <td>0.133525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.688800</td>\n",
              "      <td>3.570875</td>\n",
              "      <td>0.293853</td>\n",
              "      <td>0.126192</td>\n",
              "      <td>0.126319</td>\n",
              "      <td>0.130847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.616300</td>\n",
              "      <td>3.764277</td>\n",
              "      <td>0.297446</td>\n",
              "      <td>0.121881</td>\n",
              "      <td>0.124337</td>\n",
              "      <td>0.131326</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.673800</td>\n",
              "      <td>3.711828</td>\n",
              "      <td>0.298601</td>\n",
              "      <td>0.125619</td>\n",
              "      <td>0.128501</td>\n",
              "      <td>0.134846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.598800</td>\n",
              "      <td>3.856198</td>\n",
              "      <td>0.300141</td>\n",
              "      <td>0.126908</td>\n",
              "      <td>0.131725</td>\n",
              "      <td>0.135105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.653300</td>\n",
              "      <td>3.734875</td>\n",
              "      <td>0.289876</td>\n",
              "      <td>0.129406</td>\n",
              "      <td>0.136236</td>\n",
              "      <td>0.134191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.713800</td>\n",
              "      <td>3.786890</td>\n",
              "      <td>0.292699</td>\n",
              "      <td>0.126242</td>\n",
              "      <td>0.131591</td>\n",
              "      <td>0.134078</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='206' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [206/244 00:06 < 00:01, 30.40 it/s]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='901' max='10038' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  901/10038 03:36 < 36:43, 4.15 it/s, Epoch 0.27/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.123300</td>\n",
              "      <td>2.153698</td>\n",
              "      <td>0.280765</td>\n",
              "      <td>0.058716</td>\n",
              "      <td>0.042788</td>\n",
              "      <td>0.110905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.663500</td>\n",
              "      <td>2.347993</td>\n",
              "      <td>0.301424</td>\n",
              "      <td>0.111092</td>\n",
              "      <td>0.093399</td>\n",
              "      <td>0.148205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.204900</td>\n",
              "      <td>2.542070</td>\n",
              "      <td>0.292442</td>\n",
              "      <td>0.129700</td>\n",
              "      <td>0.131533</td>\n",
              "      <td>0.134649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.841300</td>\n",
              "      <td>2.919263</td>\n",
              "      <td>0.280252</td>\n",
              "      <td>0.125646</td>\n",
              "      <td>0.129088</td>\n",
              "      <td>0.130484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.933100</td>\n",
              "      <td>2.763415</td>\n",
              "      <td>0.325035</td>\n",
              "      <td>0.134779</td>\n",
              "      <td>0.129096</td>\n",
              "      <td>0.144039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.854700</td>\n",
              "      <td>3.038362</td>\n",
              "      <td>0.291544</td>\n",
              "      <td>0.131824</td>\n",
              "      <td>0.135085</td>\n",
              "      <td>0.149391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.907700</td>\n",
              "      <td>3.054461</td>\n",
              "      <td>0.280893</td>\n",
              "      <td>0.130696</td>\n",
              "      <td>0.136110</td>\n",
              "      <td>0.158583</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.894700</td>\n",
              "      <td>3.144430</td>\n",
              "      <td>0.320159</td>\n",
              "      <td>0.130548</td>\n",
              "      <td>0.144493</td>\n",
              "      <td>0.167257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.784900</td>\n",
              "      <td>3.264558</td>\n",
              "      <td>0.266778</td>\n",
              "      <td>0.127827</td>\n",
              "      <td>0.139442</td>\n",
              "      <td>0.128476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.805200</td>\n",
              "      <td>3.072797</td>\n",
              "      <td>0.310535</td>\n",
              "      <td>0.131206</td>\n",
              "      <td>0.125304</td>\n",
              "      <td>0.148239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.789800</td>\n",
              "      <td>3.162674</td>\n",
              "      <td>0.300141</td>\n",
              "      <td>0.131331</td>\n",
              "      <td>0.136470</td>\n",
              "      <td>0.145079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.737800</td>\n",
              "      <td>3.182196</td>\n",
              "      <td>0.287437</td>\n",
              "      <td>0.130771</td>\n",
              "      <td>0.139847</td>\n",
              "      <td>0.127508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.731100</td>\n",
              "      <td>3.285142</td>\n",
              "      <td>0.321571</td>\n",
              "      <td>0.129405</td>\n",
              "      <td>0.129061</td>\n",
              "      <td>0.168140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.795100</td>\n",
              "      <td>3.163425</td>\n",
              "      <td>0.293725</td>\n",
              "      <td>0.130360</td>\n",
              "      <td>0.137874</td>\n",
              "      <td>0.131372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.780200</td>\n",
              "      <td>3.203946</td>\n",
              "      <td>0.308354</td>\n",
              "      <td>0.129064</td>\n",
              "      <td>0.136072</td>\n",
              "      <td>0.147170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.748800</td>\n",
              "      <td>3.255240</td>\n",
              "      <td>0.289362</td>\n",
              "      <td>0.128090</td>\n",
              "      <td>0.136588</td>\n",
              "      <td>0.141573</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.693500</td>\n",
              "      <td>3.507228</td>\n",
              "      <td>0.285898</td>\n",
              "      <td>0.124887</td>\n",
              "      <td>0.136660</td>\n",
              "      <td>0.129786</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='387' max='244' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [244/244 06:11]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Checkpoint destination directory ./preds_checkpoints/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
          ]
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z53ML5vd3kqF"
      },
      "outputs": [],
      "source": [
        "q = [trainer.evaluate(eval_dataset = train_df) for train_df in [train_dataloader, val_dataloader, test_dataloader]]\n",
        "\n",
        "pd.DataFrame(q, index=['train','val','test']).iloc[:,:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42IsBkj134Av"
      },
      "outputs": [],
      "source": [
        "def predict(tweet):\n",
        "    inputs = tokenizer(tweet, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "    probs = outputs[0].softmax(1)\n",
        "\n",
        "    pred_label_idx = probs.argmax()\n",
        "    pred_label = model.config.id2label[pred_label_idx.item()]\n",
        "\n",
        "    return probs, pred_label_idx, pred_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOPY1vHF4L7E"
      },
      "outputs": [],
      "source": [
        "text = 'Some people here are severely injured we need help right now!!!'\n",
        "predict(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugiVnwCa4Ym0"
      },
      "outputs": [],
      "source": [
        "model_path = 'disaster_classifier'\n",
        "trainer.save_model(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2w06HY24nlG"
      },
      "outputs": [],
      "source": [
        "model_path = 'disaster_classifier'\n",
        "\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
        "nlp = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HMMHhAVr4wIp"
      },
      "outputs": [],
      "source": [
        "nlp('I see bodies all over the place and those seem to be corpses')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
